{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for time series analysis\n",
    "# !pip install pystac-client pandas matplotlib seaborn statsmodels pymannkendall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pystac_client import Client\n",
    "from pystac_client.stac_api_io import StacApiIO\n",
    "from datetime import datetime, timedelta\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import pymannkendall as mk\n",
    "import warnings\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from types import SimpleNamespace\n",
    "import time\n",
    "import os\n",
    "from getpass import getpass\n",
    "import json\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "AUTHENTICATION REQUIRED\n",
      "======================================================================\n",
      "\n",
      "The Montandon STAC API requires a Bearer Token for authentication.\n",
      "\n",
      "How to get your token:\n",
      "  1. Visit: https://goadmin-stage.ifrc.org/\n",
      "  2. Log in with your IFRC credentials\n",
      "  3. Generate an API token from your account settings\n",
      "\n",
      "Alternatively, set the MONTANDON_API_TOKEN environment variable:\n",
      "  PowerShell: $env:MONTANDON_API_TOKEN = 'your_token_here'\n",
      "  Bash: export MONTANDON_API_TOKEN='your_token_here'\n",
      "\n",
      "======================================================================\n",
      "\n",
      "[OK] Connected to: https://montandon-eoapi-stage.ifrc.org/\n",
      "[OK] API Title: stac-fastapi\n",
      "[OK] Authentication: Bearer Token (OpenID Connect)\n",
      "[OK] Auth Provider: https://goadmin-stage.ifrc.org/o/.well-known/openid-configuration\n",
      "\n",
      "[OK] Connected to: https://montandon-eoapi-stage.ifrc.org/\n",
      "[OK] API Title: stac-fastapi\n",
      "[OK] Authentication: Bearer Token (OpenID Connect)\n",
      "[OK] Auth Provider: https://goadmin-stage.ifrc.org/o/.well-known/openid-configuration\n"
     ]
    }
   ],
   "source": [
    "# Connect to Montandon STAC API with Authentication\n",
    "STAC_API_URL = \"https://montandon-eoapi-stage.ifrc.org/\"\n",
    "\n",
    "# Get authentication token\n",
    "# Option 1: From environment variable (recommended for automation)\n",
    "api_token = os.getenv('MONTANDON_API_TOKEN')\n",
    "\n",
    "# Option 2: Prompt user for token if not in environment\n",
    "if api_token is None:\n",
    "    print(\"=\" * 70)\n",
    "    print(\"AUTHENTICATION REQUIRED\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\\nThe Montandon STAC API requires a Bearer Token for authentication.\")\n",
    "    print(\"\\nHow to get your token:\")\n",
    "    print(\"  1. Visit: https://goadmin-stage.ifrc.org/\")\n",
    "    print(\"  2. Log in with your IFRC credentials\")\n",
    "    print(\"  3. Generate an API token from your account settings\")\n",
    "    print(\"\\nAlternatively, set the MONTANDON_API_TOKEN environment variable:\")\n",
    "    print(\"  PowerShell: $env:MONTANDON_API_TOKEN = 'your_token_here'\")\n",
    "    print(\"  Bash: export MONTANDON_API_TOKEN='your_token_here'\")\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    \n",
    "    # Prompt for token (hidden input)\n",
    "    api_token = getpass(\"Enter your Montandon API Token: \")\n",
    "    \n",
    "    if not api_token or api_token.strip() == \"\":\n",
    "        raise ValueError(\"API token is required to access the Montandon STAC API\")\n",
    "\n",
    "# Create authentication headers\n",
    "auth_headers = {\"Authorization\": f\"Bearer {api_token}\"}\n",
    "\n",
    "# Connect to STAC API with authentication\n",
    "try:\n",
    "    client = Client.open(STAC_API_URL, headers=auth_headers)\n",
    "    print(f\"\\n[OK] Connected to: {STAC_API_URL}\")\n",
    "    print(f\"[OK] API Title: {client.title}\")\n",
    "    print(f\"[OK] Authentication: Bearer Token (OpenID Connect)\")\n",
    "    print(f\"[OK] Auth Provider: https://goadmin-stage.ifrc.org/o/.well-known/openid-configuration\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n[ERROR] Authentication failed: {e}\")\n",
    "    print(\"\\nPlease check:\")\n",
    "    print(\"  1. Your token is valid and not expired\")\n",
    "    print(\"  2. You have the correct permissions\")\n",
    "    print(\"  3. The API endpoint is accessible\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Fetching Helper with CQL2 Date Filter\n",
    "\n",
    "Using CQL2 JSON filter for efficient date-based queries (last 10 years)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis period: 2014-06-30T00:00:00Z to 2024-06-30T23:59:59Z\n",
      "Note: If response is too large, data will be fetched year-by-year automatically.\n"
     ]
    }
   ],
   "source": [
    "# Define time range for analysis (20 years may be too large - will auto-split by year)\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "END_DATE = datetime(2024, 6, 30)  # June 30, 2024 (valid date)\n",
    "START_DATE = END_DATE - relativedelta(years=10)  # 10 years ago (more manageable)\n",
    "\n",
    "# Format dates for CQL2 filter\n",
    "START_DATE_STR = START_DATE.strftime('%Y-%m-%dT00:00:00Z')\n",
    "END_DATE_STR = END_DATE.strftime('%Y-%m-%dT23:59:59Z')\n",
    "\n",
    "print(f\"Analysis period: {START_DATE_STR} to {END_DATE_STR}\")\n",
    "print(f\"Note: If response is too large, data will be fetched year-by-year automatically.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bounding Boxes (approximate): [min_lon, min_lat, max_lon, max_lat]\n",
    "# Europe: -10 (W) to 40 (E), 35 (S) to 71 (N)\n",
    "# EUROPE_BBOX = [-10, 35, 40, 71]\n",
    "\n",
    "# South Asia (Pakistan, India, Bangladesh, and surrounding regions)\n",
    "# Covers: 60 (W) to 100 (E), 5 (S) to 37 (N)\n",
    "SOUTH_ASIA_BBOX = [60, 5, 100, 37]\n",
    "\n",
    "# Set the active bounding box for analysis\n",
    "ACTIVE_BBOX = SOUTH_ASIA_BBOX  # Change to EUROPE_BBOX for Europe analysis\n",
    "\n",
    "def build_datetime_filter(start_date=START_DATE_STR, end_date=END_DATE_STR):\n",
    "    \"\"\"\n",
    "    Build CQL2 JSON filter for datetime range.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"op\": \"t_intersects\",\n",
    "        \"args\": [\n",
    "            {\"property\": \"datetime\"},\n",
    "            {\"interval\": [start_date, end_date]}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def build_bbox_filter(bbox=ACTIVE_BBOX):\n",
    "    \"\"\"\n",
    "    Build CQL2 JSON filter for bounding box (spatial filter).\n",
    "    bbox format: [min_lon, min_lat, max_lon, max_lat]\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"op\": \"s_intersects\",\n",
    "        \"args\": [\n",
    "            {\"property\": \"geometry\"},\n",
    "            {\n",
    "                \"type\": \"Polygon\",\n",
    "                \"coordinates\": [[\n",
    "                    [bbox[0], bbox[1]],\n",
    "                    [bbox[2], bbox[1]],\n",
    "                    [bbox[2], bbox[3]],\n",
    "                    [bbox[0], bbox[3]],\n",
    "                    [bbox[0], bbox[1]]\n",
    "                ]]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def build_combined_filter(start_date=START_DATE_STR, end_date=END_DATE_STR, bbox=ACTIVE_BBOX):\n",
    "    \"\"\"\n",
    "    Build combined CQL2 filter for both datetime and spatial (bbox) constraints.\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"op\": \"and\",\n",
    "        \"args\": [\n",
    "            build_datetime_filter(start_date, end_date),\n",
    "            build_bbox_filter(bbox)\n",
    "        ]\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search functions initialized (with retry logic and rate limiting)\n"
     ]
    }
   ],
   "source": [
    "def search_stac_direct(collections, bbox=None, \n",
    "                       datetime_range=None, \n",
    "                       cql2_filter=None, limit=5000,\n",
    "                       max_retries=3, retry_delay=2):\n",
    "    \"\"\"\n",
    "    Search STAC API using client._stac_io for authenticated access.\n",
    "    This bypasses pystac_client's internal link resolution which causes errors.\n",
    "    \n",
    "    Includes retry logic with exponential backoff for \n",
    "    handling intermittent errors.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    collections : list\n",
    "        Collection IDs to search\n",
    "    bbox : list\n",
    "        Bounding box [min_lon, min_lat, max_lon, max_lat]\n",
    "    datetime_range : str\n",
    "        ISO 8601 datetime range string \n",
    "        (e.g., '2024-01-01/2024-12-31')\n",
    "    cql2_filter : dict\n",
    "        CQL2 filter body (optional, may not be supported)\n",
    "    limit : int\n",
    "        Maximum number of results\n",
    "    max_retries : int\n",
    "        Maximum number of retry attempts (default: 3)\n",
    "    retry_delay : int\n",
    "        Base delay in seconds between retries (exponential backoff applied)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    list of SimpleNamespace objects\n",
    "        Items with .id, .collection_id, .properties (dict), .geometry\n",
    "    \"\"\"\n",
    "    search_url = f\"{STAC_API_URL}search\"\n",
    "    \n",
    "    # Build search payload using standard STAC search parameters\n",
    "    search_payload = {\"limit\": limit}\n",
    "    \n",
    "    if collections:\n",
    "        search_payload[\"collections\"] = collections\n",
    "    if bbox:\n",
    "        search_payload[\"bbox\"] = bbox\n",
    "    if datetime_range:\n",
    "        search_payload[\"datetime\"] = datetime_range\n",
    "    \n",
    "    # Only add CQL2 filter if explicitly provided (may cause errors)\n",
    "    if cql2_filter:\n",
    "        search_payload[\"filter\"] = cql2_filter\n",
    "        search_payload[\"filter-lang\"] = \"cql2-json\"\n",
    "    \n",
    "    # Retry logic with exponential backoff\n",
    "    last_error = None\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Use client._stac_io.request() for authenticated POST request\n",
    "            response_text = client._stac_io.request(\n",
    "                search_url,\n",
    "                method=\"POST\",\n",
    "                headers={\"Content-Type\": \"application/json\"},\n",
    "                parameters=search_payload\n",
    "            )\n",
    "            \n",
    "            # Parse the JSON response\n",
    "            response_data = json.loads(response_text)\n",
    "            \n",
    "            # Convert features to SimpleNamespace objects for attribute access\n",
    "            items = []\n",
    "            for feature in response_data.get(\"features\", []):\n",
    "                item = SimpleNamespace(\n",
    "                    id=feature.get(\"id\"),\n",
    "                    collection_id=feature.get(\"collection\"),\n",
    "                    geometry=feature.get(\"geometry\"),\n",
    "                    bbox=feature.get(\"bbox\"),\n",
    "                    properties=feature.get(\"properties\", {}),\n",
    "                    links=feature.get(\"links\", []),\n",
    "                    assets=feature.get(\"assets\", {})\n",
    "                )\n",
    "                items.append(item)\n",
    "            \n",
    "            return items\n",
    "            \n",
    "        except Exception as e:\n",
    "            last_error = e\n",
    "            error_msg = str(e)\n",
    "            \n",
    "            # Don't retry for certain error types\n",
    "            if \"ProgramLimitExceededError\" in error_msg:\n",
    "                raise  # Let the caller handle this\n",
    "            \n",
    "            # Retry for server errors and timeouts\n",
    "            if attempt < max_retries - 1:\n",
    "                wait_time = retry_delay * (2 ** attempt)  # Exponential backoff\n",
    "                print(f\"      ‚ö†Ô∏è  Attempt {attempt + 1} failed: {error_msg[:50]}... Retrying in {wait_time}s\")\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                raise  # Re-raise on final attempt\n",
    "    \n",
    "    raise last_error\n",
    "\n",
    "def fetch_items_with_filter(collections, bbox=None, datetime_range=None, cql2_filter=None, limit=5000):\n",
    "    \"\"\"\n",
    "    Fetch items from specified collections using standard STAC search parameters.\n",
    "    Uses client._stac_io for authenticated API access with retry logic.\n",
    "    \"\"\"\n",
    "    print(f\"Fetching items from {collections}...\")\n",
    "    \n",
    "    if datetime_range:\n",
    "        print(f\"   Datetime: {datetime_range}\")\n",
    "    if bbox:\n",
    "        print(f\"   Bbox: {bbox}\")\n",
    "    \n",
    "    try:\n",
    "        items = search_stac_direct(collections, bbox=bbox, datetime_range=datetime_range, \n",
    "                                    cql2_filter=cql2_filter, limit=limit)\n",
    "        print(f\"‚úÖ Found {len(items)} items.\")\n",
    "        return items\n",
    "    except Exception as e:\n",
    "        error_msg = str(e)\n",
    "        \n",
    "        # Handle \"ProgramLimitExceededError\" - response too large\n",
    "        if \"ProgramLimitExceededError\" in error_msg or \"exceeds the maximum\" in error_msg:\n",
    "            print(\"Response too large! Splitting request by year...\")\n",
    "            return fetch_items_by_year(collections, bbox, datetime_range)\n",
    "        \n",
    "        # Handle TimeoutError\n",
    "        elif \"TimeoutError\" in error_msg or \"timeout\" in error_msg.lower():\n",
    "            print(\"TIMEOUT: Retrying with smaller limit (2000)...\")\n",
    "            try:\n",
    "                items = search_stac_direct(collections, bbox=bbox, datetime_range=datetime_range,\n",
    "                                           cql2_filter=cql2_filter, limit=2000)\n",
    "                print(f\"‚úÖ Found {len(items)} items (with reduced limit).\")\n",
    "                return items\n",
    "            except Exception as e2:\n",
    "                print(f\"Error even with reduced limit: {str(e2)}\")\n",
    "                return []\n",
    "        else:\n",
    "            print(f\"Error fetching items: {error_msg}\")\n",
    "            return []\n",
    "\n",
    "print(\"Search functions initialized (with retry logic and rate limiting)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions initialized (year-by-year with PAGINATION, page_size=100)\n"
     ]
    }
   ],
   "source": [
    "def fetch_items_by_year_paginated(collections, bbox=None, \n",
    "                                   start_year=None, end_year=None, \n",
    "                                   page_size=100, \n",
    "                                   delay_between_requests=0.5,\n",
    "                                   max_pages_per_year=100):\n",
    "    \"\"\"\n",
    "    Fetch ALL items year-by-year with PAGINATION for maximum reliability.\n",
    "    \n",
    "    This function fetches ALL records for each year by making multiple\n",
    "    paginated requests until no more data is available.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    collections : list\n",
    "        Collection IDs to search\n",
    "    bbox : list\n",
    "        Bounding box [min_lon, min_lat, max_lon, max_lat]\n",
    "    start_year : int\n",
    "        Starting year (defaults to START_DATE.year)\n",
    "    end_year : int\n",
    "        Ending year (defaults to END_DATE.year)\n",
    "    page_size : int\n",
    "        Records per API request/page (default: 100 - small for reliability)\n",
    "    delay_between_requests : float\n",
    "        Delay in seconds between API requests (default: 0.5s)\n",
    "    max_pages_per_year : int\n",
    "        Safety limit to prevent infinite loops (default: 100 = 10,000 records max per year)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    list of SimpleNamespace objects\n",
    "        All items fetched with pagination\n",
    "    \"\"\"\n",
    "    all_items = []\n",
    "    \n",
    "    if start_year is None:\n",
    "        start_year = START_DATE.year\n",
    "    if end_year is None:\n",
    "        end_year = END_DATE.year\n",
    "    \n",
    "    print(f\"   Fetching year-by-year WITH PAGINATION ({start_year}-{end_year})\")\n",
    "    print(f\"   Page size: {page_size} records per request\")\n",
    "    \n",
    "    for current_year in range(start_year, end_year + 1):\n",
    "        # Create datetime range for this year\n",
    "        year_start_str = f\"{current_year}-01-01T00:00:00Z\"\n",
    "        year_end_str = f\"{current_year}-12-31T23:59:59Z\"\n",
    "        datetime_range = f\"{year_start_str}/{year_end_str}\"\n",
    "        \n",
    "        print(f\"     {current_year}: \", end=\"\", flush=True)\n",
    "        \n",
    "        year_items = []\n",
    "        page_num = 0\n",
    "        \n",
    "        try:\n",
    "            # Keep fetching pages until we get fewer items than page_size\n",
    "            while page_num < max_pages_per_year:\n",
    "                page_items = search_stac_direct(\n",
    "                    collections, \n",
    "                    bbox=bbox, \n",
    "                    datetime_range=datetime_range, \n",
    "                    limit=page_size\n",
    "                )\n",
    "                \n",
    "                if not page_items:\n",
    "                    # No more items\n",
    "                    break\n",
    "                \n",
    "                year_items.extend(page_items)\n",
    "                page_num += 1\n",
    "                \n",
    "                # If we got fewer items than page_size, we've reached the end\n",
    "                if len(page_items) < page_size:\n",
    "                    break\n",
    "                \n",
    "                # Rate limiting between pages\n",
    "                time.sleep(delay_between_requests)\n",
    "            \n",
    "            all_items.extend(year_items)\n",
    "            \n",
    "            if page_num > 1:\n",
    "                print(f\"‚úÖ {len(year_items)} ({page_num} pages)\")\n",
    "            else:\n",
    "                print(f\"‚úÖ {len(year_items)}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Keep partial data if we got some\n",
    "            if year_items:\n",
    "                all_items.extend(year_items)\n",
    "                print(f\"‚ö†Ô∏è {len(year_items)} partial ({str(e)[:25]}...)\")\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è 0 ({str(e)[:30]}...)\")\n",
    "        \n",
    "        # Rate limiting between years\n",
    "        time.sleep(delay_between_requests)\n",
    "    \n",
    "    print(f\"   Total items fetched: {len(all_items)}\")\n",
    "    return all_items\n",
    "\n",
    "\n",
    "def items_to_dataframe(items):\n",
    "    \"\"\"Convert STAC items (SimpleNamespace objects) to a pandas DataFrame.\"\"\"\n",
    "    data = []\n",
    "    for item in items:\n",
    "        props = item.properties\n",
    "        hazard_codes = props.get(\"monty:hazard_codes\", [])\n",
    "        country_codes = props.get(\"monty:country_codes\", [])\n",
    "        primary_country = country_codes[0] if country_codes else \"Unknown\"\n",
    "        entry = {\n",
    "            \"id\": item.id,\n",
    "            \"datetime\": pd.to_datetime(props.get(\"datetime\") or props.get(\"start_datetime\")),\n",
    "            \"title\": props.get(\"title\"),\n",
    "            \"hazard_codes\": hazard_codes,\n",
    "            \"primary_country\": primary_country,\n",
    "            \"collection\": item.collection_id\n",
    "        }\n",
    "        data.append(entry)\n",
    "    df = pd.DataFrame(data)\n",
    "    if not df.empty:\n",
    "        df.set_index(\"datetime\", inplace=True)\n",
    "        df.sort_index(inplace=True)\n",
    "    return df\n",
    "\n",
    "print(\"Helper functions initialized (year-by-year with PAGINATION, page_size=100)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study 2.1: Seasonality Patterns by Hazard Type\n",
    "\n",
    "We analyze seasonality of **Floods** and **Tropical Cyclones**.\n",
    "\n",
    "**Hypothesis:**\n",
    "- Tropical Cyclones show distinct seasonal peaks (hurricane season).\n",
    "- Floods show seasonality linked to monsoon seasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "FETCHING EVENTS DATA (YEAR-BY-YEAR WITH PAGINATION)\n",
      "======================================================================\n",
      "Collections: ['gdacs-events', 'emdat-events', 'glide-events']\n",
      "Region: South Asia (bbox: [60, 5, 100, 37])\n",
      "Time Range: 2014 to 2024\n",
      "Page size: 100 records per request (fetches ALL pages per year)\n",
      "\n",
      "üì¶ gdacs-events\n",
      "   Fetching year-by-year WITH PAGINATION (2014-2024)\n",
      "   Page size: 100 records per request\n",
      "     2014: ‚úÖ 20\n",
      "‚úÖ 20\n",
      "     2015:      2015: ‚úÖ 14\n",
      "‚úÖ 14\n",
      "     2016:      2016: ‚úÖ 13\n",
      "‚úÖ 13\n",
      "     2017:      2017: ‚úÖ 0\n",
      "‚úÖ 0\n",
      "     2018:      2018: ‚úÖ 0\n",
      "‚úÖ 0\n",
      "     2019:      2019: ‚úÖ 23\n",
      "‚úÖ 23\n",
      "     2020:      2020: ‚úÖ 46\n",
      "‚úÖ 46\n",
      "     2021:      2021: ‚úÖ 10000 (100 pages)\n",
      "‚úÖ 10000 (100 pages)\n",
      "     2022:      2022: ‚úÖ 10000 (100 pages)\n",
      "‚úÖ 10000 (100 pages)\n",
      "     2023:      2023: ‚úÖ 10000 (100 pages)\n",
      "‚úÖ 10000 (100 pages)\n",
      "     2024:      2024: ‚úÖ 60\n",
      "‚úÖ 60\n",
      "   Total items fetched: 30176\n",
      "   Total items fetched: 30176\n",
      "\n",
      "üì¶ emdat-events\n",
      "   Fetching year-by-year WITH PAGINATION (2014-2024)\n",
      "   Page size: 100 records per request\n",
      "     2014: \n",
      "üì¶ emdat-events\n",
      "   Fetching year-by-year WITH PAGINATION (2014-2024)\n",
      "   Page size: 100 records per request\n",
      "     2014: "
     ]
    }
   ],
   "source": [
    "# Fetch Data for Seasonality Analysis (last 10 years, South Asia) - WITH PAGINATION\n",
    "collections_of_interest = [\"gdacs-events\", \"emdat-events\", \"glide-events\"]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"FETCHING EVENTS DATA (YEAR-BY-YEAR WITH PAGINATION)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Collections: {collections_of_interest}\")\n",
    "print(f\"Region: South Asia (bbox: {ACTIVE_BBOX})\")\n",
    "print(f\"Time Range: {START_DATE.year} to {END_DATE.year}\")\n",
    "print(f\"Page size: 100 records per request (fetches ALL pages per year)\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "all_events = []\n",
    "collection_stats = {}\n",
    "\n",
    "# Fetch each collection year-by-year with pagination (SEQUENTIAL)\n",
    "for collection in collections_of_interest:\n",
    "    print(f\"\\nüì¶ {collection}\")\n",
    "    \n",
    "    try:\n",
    "        # Use paginated function to get ALL records\n",
    "        collection_items = fetch_items_by_year_paginated(\n",
    "            [collection],\n",
    "            bbox=ACTIVE_BBOX,\n",
    "            start_year=START_DATE.year,\n",
    "            end_year=END_DATE.year,\n",
    "            page_size=100,  # 100 records per page (will paginate for more)\n",
    "            delay_between_requests=0.5\n",
    "        )\n",
    "        all_events.extend(collection_items)\n",
    "        collection_stats[collection] = len(collection_items)\n",
    "    except Exception as e:\n",
    "        collection_stats[collection] = 0\n",
    "        print(f\"   ‚ö†Ô∏è Failed: {str(e)[:50]}...\")\n",
    "    \n",
    "    time.sleep(1.0)  # Delay between collections\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(f\"‚úÖ Fetch complete in {elapsed_time:.1f}s\")\n",
    "print(f\"Total: {len(all_events)} items\")\n",
    "for coll, count in collection_stats.items():\n",
    "    print(f\"  {coll}: {count}\")\n",
    "\n",
    "# Convert to dataframe\n",
    "df_events = items_to_dataframe(all_events)\n",
    "print(f\"\\nDataFrame: {df_events.shape[0]} rows, {df_events.shape[1]} columns\")\n",
    "\n",
    "if not df_events.empty:\n",
    "    display(df_events.head())\n",
    "    print(f\"\\nDate range: {df_events.index.min()} to {df_events.index.max()}\")\n",
    "    print(f\"Countries: {df_events['primary_country'].nunique()}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for Specific Hazards\n",
    "# Including GLIDE, EM-DAT, and UNDRR-ISC format codes\n",
    "\n",
    "def filter_hazard(df, codes_list):\n",
    "    mask = df['hazard_codes'].apply(lambda x: any(code in x for code in codes_list))\n",
    "    return df[mask]\n",
    "\n",
    "# CORRECTED hazard codes per official documentation\n",
    "flood_codes = [\"FL\", \"FF\", \"MH0600\", \"MH0601\", \"MH0602\", \"MH0603\", \"MH0604\",\n",
    "               \"nat-hyd-flo-flo\", \"nat-hyd-flo-fla\", \"nat-hyd-flo-riv\", \"nat-hyd-flo-coa\"]\n",
    "cyclone_codes = [\"TC\", \"MH0306\", \"MH0307\", \"MH0308\", \"MH0309\", \"nat-met-sto-tro\"]\n",
    "\n",
    "df_floods = filter_hazard(df_events, flood_codes)\n",
    "df_cyclones = filter_hazard(df_events, cyclone_codes)\n",
    "print(f\"Floods: {len(df_floods)}, Cyclones: {len(df_cyclones)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Monthly Distribution\n",
    "def plot_monthly_seasonality(df, title, color):\n",
    "    if df.empty:\n",
    "        print(f\"No data for {title}\")\n",
    "        return\n",
    "    df_copy = df.copy()\n",
    "    df_copy['month'] = df_copy.index.month\n",
    "    monthly_counts = df_copy.groupby('month').size().reindex(range(1,13), fill_value=0)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.barplot(x=monthly_counts.index, y=monthly_counts.values, color=color)\n",
    "    plt.title(f\"Seasonal Distribution: {title}\")\n",
    "    plt.xlabel(\"Month\")\n",
    "    plt.ylabel(\"Number of Events\")\n",
    "    plt.xticks(range(0,12), ['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec'])\n",
    "    plt.show()\n",
    "\n",
    "plot_monthly_seasonality(df_floods, \"Floods\", \"skyblue\")\n",
    "plot_monthly_seasonality(df_cyclones, \"Tropical Cyclones\", \"teal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Series Decomposition (STL)\n",
    "\n",
    "We decompose the data into **Trend**, **Seasonality**, and **Residuals**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_time_series(df, title, freq='M'):\n",
    "    if df.empty:\n",
    "        return\n",
    "    ts = df.resample(freq).size().fillna(0)\n",
    "    if len(ts) < 24:\n",
    "        print(f\"Not enough data for {title} (need 24+ months, got {len(ts)})\")\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        ts.plot()\n",
    "        plt.title(f\"Time Series: {title}\")\n",
    "        plt.show()\n",
    "        return\n",
    "    decomposition = seasonal_decompose(ts, model='additive', period=12)\n",
    "    fig = decomposition.plot()\n",
    "    fig.set_size_inches(12, 10)\n",
    "    fig.suptitle(f'STL Decomposition: {title}', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "decompose_time_series(df_floods, \"Flood Events (Monthly)\")\n",
    "decompose_time_series(df_cyclones, \"Cyclone Events (Monthly)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Study 2.2: Impact Severity Trends\n",
    "\n",
    "Analyze whether **impact severity** is increasing or decreasing over time.\n",
    "\n",
    "**Methodology:**\n",
    "1. Fetch all impact collections (last 10 years)\n",
    "2. Extract `monty:impact_detail` values\n",
    "3. Apply **Mann-Kendall Trend Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch ALL Impact Data (last 10 years, South Asia) - WITH PAGINATION\n",
    "impact_collections = [\"gdacs-impacts\", \"emdat-impacts\", \"desinventar-impacts\",\n",
    "                      \"idmc-gidd-impacts\", \"idmc-idu-impacts\"]\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"FETCHING IMPACT DATA (YEAR-BY-YEAR WITH PAGINATION)\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Collections: {impact_collections}\")\n",
    "print(f\"Region: South Asia (bbox: {ACTIVE_BBOX})\")\n",
    "print(f\"Time Range: {START_DATE.year} to {END_DATE.year}\")\n",
    "print(f\"Page size: 100 records per request (fetches ALL pages per year)\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "all_impacts = []\n",
    "impact_stats = {}\n",
    "\n",
    "# Fetch each collection year-by-year with pagination (SEQUENTIAL)\n",
    "for collection in impact_collections:\n",
    "    print(f\"\\nüì¶ {collection}\")\n",
    "    \n",
    "    try:\n",
    "        # Use paginated function to get ALL records\n",
    "        collection_items = fetch_items_by_year_paginated(\n",
    "            [collection],\n",
    "            bbox=ACTIVE_BBOX,\n",
    "            start_year=START_DATE.year,\n",
    "            end_year=END_DATE.year,\n",
    "            page_size=100,  # 100 records per page (will paginate for more)\n",
    "            delay_between_requests=0.5\n",
    "        )\n",
    "        all_impacts.extend(collection_items)\n",
    "        impact_stats[collection] = len(collection_items)\n",
    "    except Exception as e:\n",
    "        impact_stats[collection] = 0\n",
    "        print(f\"   ‚ö†Ô∏è Failed: {str(e)[:50]}...\")\n",
    "    \n",
    "    time.sleep(1.0)  # Extra delay between collections\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n{'=' * 70}\")\n",
    "print(f\"‚úÖ Fetch complete in {elapsed_time:.1f}s\")\n",
    "print(f\"Total: {len(all_impacts)} impact items\")\n",
    "for coll, count in impact_stats.items():\n",
    "    print(f\"  {coll}: {count}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Impact Details\n",
    "def process_impacts(items):\n",
    "    data = []\n",
    "    for item in items:\n",
    "        props = item.properties\n",
    "        impact_detail = props.get(\"monty:impact_detail\", {})\n",
    "        if not impact_detail:\n",
    "            continue\n",
    "        entry = {\n",
    "            \"datetime\": pd.to_datetime(props.get(\"datetime\") or props.get(\"start_datetime\")),\n",
    "            \"category\": impact_detail.get(\"category\"),\n",
    "            \"type\": impact_detail.get(\"type\"),\n",
    "            \"value\": impact_detail.get(\"value\"),\n",
    "            \"unit\": impact_detail.get(\"unit\"),\n",
    "            \"country\": props.get(\"monty:country_codes\", [\"Unknown\"])[0]\n",
    "        }\n",
    "        data.append(entry)\n",
    "    df = pd.DataFrame(data)\n",
    "    if not df.empty:\n",
    "        df['value'] = pd.to_numeric(df['value'], errors='coerce')\n",
    "        df.dropna(subset=['value'], inplace=True)\n",
    "        df.set_index(\"datetime\", inplace=True)\n",
    "        df.sort_index(inplace=True)\n",
    "    return df\n",
    "\n",
    "df_impacts = process_impacts(all_impacts)\n",
    "display(df_impacts.head())\n",
    "display(df_impacts['type'].value_counts().head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Trends (using np.polyfit, not deprecated pd.np)\n",
    "def analyze_trend(df, impact_type, title):\n",
    "    \"\"\"\n",
    "    Analyze trend for a specific impact type with visualization and statistical test.\n",
    "    \"\"\"\n",
    "    subset = df[df['type'] == impact_type].copy()\n",
    "    if subset.empty:\n",
    "        print(f\"‚ùå No data for '{impact_type}'\")\n",
    "        return\n",
    "    \n",
    "    # Aggregate by year\n",
    "    ts_yearly = subset['value'].resample('Y').sum()\n",
    "    \n",
    "    if len(ts_yearly) < 2:\n",
    "        print(f\"‚ö†Ô∏è  Not enough yearly data for '{impact_type}' (need at least 2 years)\")\n",
    "        return\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 5))\n",
    "    \n",
    "    # Plot 1: Yearly trend\n",
    "    ax1.plot(ts_yearly.index.year, ts_yearly.values, marker='o', linewidth=2, markersize=8, color='steelblue')\n",
    "    ax1.set_title(f\"Yearly Trend: {title}\", fontsize=14, fontweight='bold')\n",
    "    ax1.set_ylabel(\"Total Value\", fontsize=12)\n",
    "    ax1.set_xlabel(\"Year\", fontsize=12)\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 2: Monthly trend (if enough data)\n",
    "    ts_monthly = subset['value'].resample('M').sum()\n",
    "    ax2.plot(ts_monthly.index, ts_monthly.values, linewidth=1.5, color='darkgreen', alpha=0.7)\n",
    "    ax2.fill_between(ts_monthly.index, ts_monthly.values, alpha=0.3, color='darkgreen')\n",
    "    ax2.set_title(f\"Monthly Trend: {title}\", fontsize=14, fontweight='bold')\n",
    "    ax2.set_ylabel(\"Total Value\", fontsize=12)\n",
    "    ax2.set_xlabel(\"Date\", fontsize=12)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üìä IMPACT ANALYSIS: {title.upper()}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Total records: {len(subset)}\")\n",
    "    print(f\"Year range: {subset.index.year.min()} - {subset.index.year.max()}\")\n",
    "    print(f\"Total value: {subset['value'].sum():,.0f}\")\n",
    "    print(f\"Average per event: {subset['value'].mean():,.2f}\")\n",
    "    print(f\"Max value: {subset['value'].max():,.0f}\")\n",
    "    print(f\"Min value: {subset['value'].min():,.0f}\")\n",
    "    \n",
    "    # Perform Mann-Kendall trend test\n",
    "    if len(ts_yearly) >= 3:\n",
    "        result = mk.original_test(ts_yearly.values)\n",
    "        trend_interpretation = \"üìà INCREASING\" if result.trend == \"increasing\" else \"üìâ DECREASING\" if result.trend == \"decreasing\" else \"‚û°Ô∏è  NO TREND\"\n",
    "        print(f\"\\n{trend_interpretation} TREND (Mann-Kendall Test)\")\n",
    "        print(f\"  P-value: {result.p:.4f} {'(Significant at 0.05)' if result.p < 0.05 else '(Not significant)'}\")\n",
    "        print(f\"  Slope: {result.slope:.4f} per year\")\n",
    "        print(f\"  S-statistic: {result.s}\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è  Not enough yearly data for Mann-Kendall test (need at least 3 years, got {len(ts_yearly)})\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Analyze the top impact types from the data\n",
    "print(\"üîç ANALYZING TOP IMPACT TYPES FROM SOUTH ASIA DATA\\n\")\n",
    "impact_types_to_analyze = ['displaced_internal', 'affected_total', 'death', 'shelter_emergency', 'evacuated', 'injured']\n",
    "\n",
    "for impact_type in impact_types_to_analyze:\n",
    "    analyze_trend(df_impacts, impact_type, impact_type.replace('_', ' ').title())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Event Durations from the existing df_events DataFrame\n",
    "print(\"=\" * 70)\n",
    "print(\"EVENT DURATION ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# We need to re-fetch events with start/end datetime properties\n",
    "# Let's create a more detailed dataframe extraction function\n",
    "def items_to_duration_dataframe(items):\n",
    "    \"\"\"Convert STAC items to DataFrame with duration information.\"\"\"\n",
    "    data = []\n",
    "    for item in items:\n",
    "        props = item.properties\n",
    "        \n",
    "        # Extract datetime information\n",
    "        datetime_val = props.get(\"datetime\")\n",
    "        start_datetime = props.get(\"start_datetime\")\n",
    "        end_datetime = props.get(\"end_datetime\")\n",
    "        \n",
    "        # Parse dates\n",
    "        if start_datetime and end_datetime:\n",
    "            start = pd.to_datetime(start_datetime)\n",
    "            end = pd.to_datetime(end_datetime)\n",
    "            duration_days = (end - start).total_seconds() / (24 * 3600)\n",
    "            event_date = start\n",
    "        elif datetime_val:\n",
    "            event_date = pd.to_datetime(datetime_val)\n",
    "            duration_days = 0  # Single point event\n",
    "            start = event_date\n",
    "            end = event_date\n",
    "        else:\n",
    "            continue  # Skip if no valid datetime\n",
    "        \n",
    "        hazard_codes = props.get(\"monty:hazard_codes\", [])\n",
    "        country_codes = props.get(\"monty:country_codes\", [])\n",
    "        primary_country = country_codes[0] if country_codes else \"Unknown\"\n",
    "        \n",
    "        entry = {\n",
    "            \"id\": item.id,\n",
    "            \"start_datetime\": start,\n",
    "            \"end_datetime\": end,\n",
    "            \"duration_days\": duration_days,\n",
    "            \"title\": props.get(\"title\"),\n",
    "            \"hazard_codes\": hazard_codes,\n",
    "            \"primary_country\": primary_country,\n",
    "            \"collection\": item.collection_id,\n",
    "            \"month\": start.month,\n",
    "            \"season\": get_season(start.month)\n",
    "        }\n",
    "        data.append(entry)\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "def get_season(month):\n",
    "    \"\"\"Determine season from month (Northern Hemisphere).\"\"\"\n",
    "    if month in [12, 1, 2]:\n",
    "        return \"Winter\"\n",
    "    elif month in [3, 4, 5]:\n",
    "        return \"Spring\"\n",
    "    elif month in [6, 7, 8]:\n",
    "        return \"Summer\"\n",
    "    else:\n",
    "        return \"Fall\"\n",
    "\n",
    "# Create duration dataframe from existing events\n",
    "df_duration = items_to_duration_dataframe(all_events)\n",
    "\n",
    "print(f\"Total events with duration data: {len(df_duration)}\")\n",
    "print(f\"\\nDuration statistics (BEFORE cleaning):\")\n",
    "print(df_duration['duration_days'].describe())\n",
    "\n",
    "# Data Quality Check: Identify problematic records\n",
    "print(f\"\\n\" + \"=\" * 70)\n",
    "print(\"DATA QUALITY CHECKS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Check for negative durations\n",
    "negative_duration = df_duration[df_duration['duration_days'] < 0]\n",
    "print(f\"\\n1. Negative Duration Events: {len(negative_duration)}\")\n",
    "if len(negative_duration) > 0:\n",
    "    print(f\"   Min: {negative_duration['duration_days'].min():.2f} days\")\n",
    "    print(f\"   Max: {negative_duration['duration_days'].max():.2f} days\")\n",
    "    display(negative_duration[['start_datetime', 'end_datetime', 'duration_days', 'primary_country', 'hazard_codes']].head())\n",
    "\n",
    "# Check for zero duration events\n",
    "zero_duration = df_duration[df_duration['duration_days'] == 0]\n",
    "print(f\"\\n2. Zero Duration Events (instantaneous): {len(zero_duration)}\")\n",
    "\n",
    "# Check for extreme outliers (>365 days = 1 year)\n",
    "extreme_duration = df_duration[df_duration['duration_days'] > 365]\n",
    "print(f\"\\n3. Extreme Duration Events (>365 days): {len(extreme_duration)}\")\n",
    "if len(extreme_duration) > 0:\n",
    "    print(f\"   Max: {extreme_duration['duration_days'].max():.2f} days ({extreme_duration['duration_days'].max()/365:.1f} years)\")\n",
    "\n",
    "# Clean the data: Keep only valid positive durations (>=0 and <=365 days)\n",
    "# This removes:\n",
    "# - Negative durations (data errors)\n",
    "# - Extreme outliers (>1 year is suspicious for most disaster types)\n",
    "df_duration_clean = df_duration[(df_duration['duration_days'] >= 0) & (df_duration['duration_days'] <= 365)].copy()\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 70)\n",
    "print(\"CLEANING RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Original records: {len(df_duration)}\")\n",
    "print(f\"Records removed: {len(df_duration) - len(df_duration_clean)}\")\n",
    "print(f\"Cleaned records: {len(df_duration_clean)}\")\n",
    "print(f\"Data retention rate: {(len(df_duration_clean)/len(df_duration)*100):.1f}%\")\n",
    "\n",
    "print(f\"\\n\\nDuration statistics (AFTER cleaning):\")\n",
    "print(df_duration_clean['duration_days'].describe())\n",
    "\n",
    "# Display cleaned sample\n",
    "print(f\"\\nSample of cleaned data:\")\n",
    "display(df_duration_clean.head(10))\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Use cleaned dataset for all subsequent analyses\n",
    "df_duration = df_duration_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# DIAGNOSTIC: Explicit Data Cleaning Verification\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üîç DIAGNOSTIC: DATA CLEANING VERIFICATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Show current state of df_duration\n",
    "print(f\"\\nüìä Current df_duration statistics:\")\n",
    "print(f\"  Shape: {df_duration.shape}\")\n",
    "print(f\"  Min duration: {df_duration['duration_days'].min():.2f} days\")\n",
    "print(f\"  Max duration: {df_duration['duration_days'].max():.2f} days\")\n",
    "print(f\"  Negative values: {(df_duration['duration_days'] < 0).sum()}\")\n",
    "print(f\"  Values > 365 days: {(df_duration['duration_days'] > 365).sum()}\")\n",
    "\n",
    "# If negative values exist, FORCE clean immediately\n",
    "if (df_duration['duration_days'] < 0).sum() > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è  ALERT: Found negative durations! Forcing clean now...\")\n",
    "    df_duration = df_duration[(df_duration['duration_days'] >= 0) & (df_duration['duration_days'] <= 365)].copy()\n",
    "    print(f\"‚úÖ After cleaning:\")\n",
    "    print(f\"  Records kept: {len(df_duration)}\")\n",
    "    print(f\"  Min: {df_duration['duration_days'].min():.2f} days\")\n",
    "    print(f\"  Max: {df_duration['duration_days'].max():.2f} days\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Data Cleaning\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DATA CLEANING VERIFICATION\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nDataFrame info:\")\n",
    "print(f\"  Total records: {len(df_duration)}\")\n",
    "print(f\"  Min duration: {df_duration['duration_days'].min():.2f} days\")\n",
    "print(f\"  Max duration: {df_duration['duration_days'].max():.2f} days\")\n",
    "print(f\"  Records with negative duration: {(df_duration['duration_days'] < 0).sum()}\")\n",
    "print(f\"  Records with duration > 365 days: {(df_duration['duration_days'] > 365).sum()}\")\n",
    "\n",
    "# If still seeing negative values, show them\n",
    "if (df_duration['duration_days'] < 0).sum() > 0:\n",
    "    print(f\"\\n‚ö†Ô∏è  WARNING: Found {(df_duration['duration_days'] < 0).sum()} negative duration records!\")\n",
    "    print(\"Removing them now...\")\n",
    "    df_duration = df_duration[df_duration['duration_days'] >= 0].copy()\n",
    "    print(f\"After removal: {len(df_duration)} records\")\n",
    "    print(f\"New min: {df_duration['duration_days'].min():.2f} days\")\n",
    "\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Classify Hazards as Sudden-Onset vs Slow-Onset\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"HAZARD CLASSIFICATION: SUDDEN-ONSET vs SLOW-ONSET\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Define hazard categories\n",
    "sudden_onset_codes = [\n",
    "    \"TC\", \"EQ\", \"TS\", \"VO\",  # GLIDE codes: Cyclone, Earthquake, Tsunami, Volcano\n",
    "    \"MH0306\", \"MH0307\", \"MH0308\", \"MH0309\",  # UNDRR Tropical cyclones\n",
    "    \"MH0100\", \"MH0101\", \"MH0102\",  # UNDRR Earthquakes\n",
    "    \"nat-met-sto-tro\",  # EM-DAT Tropical storms\n",
    "    \"nat-geo-tec-ear\",  # EM-DAT Earthquakes\n",
    "]\n",
    "\n",
    "slow_onset_codes = [\n",
    "    \"FL\", \"FF\", \"DR\", \"CW\", \"HT\",  # GLIDE: Floods, Drought, Cold wave, Heat wave\n",
    "    \"MH0600\", \"MH0601\", \"MH0602\", \"MH0603\", \"MH0604\",  # UNDRR Floods\n",
    "    \"MH0800\", \"MH0801\", \"MH0802\",  # UNDRR Droughts\n",
    "    \"nat-hyd-flo-flo\", \"nat-hyd-flo-fla\", \"nat-hyd-flo-riv\", \"nat-hyd-flo-coa\",  # EM-DAT Floods\n",
    "    \"nat-cli-dro\", \"nat-cli-ext-hea\", \"nat-cli-ext-col\",  # EM-DAT Droughts, Extreme temps\n",
    "]\n",
    "\n",
    "def classify_onset_type(hazard_codes):\n",
    "    \"\"\"Classify event as sudden or slow onset.\"\"\"\n",
    "    for code in hazard_codes:\n",
    "        if any(sudden in code for sudden in sudden_onset_codes):\n",
    "            return \"Sudden-Onset\"\n",
    "        if any(slow in code for slow in slow_onset_codes):\n",
    "            return \"Slow-Onset\"\n",
    "    return \"Other\"\n",
    "\n",
    "df_duration['onset_type'] = df_duration['hazard_codes'].apply(classify_onset_type)\n",
    "\n",
    "# Distribution by onset type\n",
    "onset_distribution = df_duration['onset_type'].value_counts()\n",
    "print(f\"\\nOnset Type Distribution:\")\n",
    "print(onset_distribution)\n",
    "\n",
    "# Duration by onset type\n",
    "print(f\"\\nAverage Duration by Onset Type (days):\")\n",
    "duration_by_onset = df_duration.groupby('onset_type')['duration_days'].agg(['mean', 'median', 'std', 'count', 'min', 'max'])\n",
    "print(duration_by_onset)\n",
    "\n",
    "# Visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Duration distribution by onset type with better scaling\n",
    "df_duration.boxplot(column='duration_days', by='onset_type', ax=ax1)\n",
    "ax1.set_title('Event Duration by Onset Type', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Onset Type', fontsize=12)\n",
    "ax1.set_ylabel('Duration (days)', fontsize=12)\n",
    "# Auto-scale to show all data with padding\n",
    "max_duration = df_duration['duration_days'].max()\n",
    "ax1.set_ylim(-5, max_duration * 1.05)  # Add 5% padding at top\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "plt.suptitle('')  # Remove automatic title\n",
    "\n",
    "# Plot 2: Count by onset type\n",
    "onset_distribution.plot(kind='bar', ax=ax2, color=['#FF6B6B', '#4ECDC4', '#95E1D3'])\n",
    "ax2.set_title('Event Count by Onset Type', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Onset Type', fontsize=12)\n",
    "ax2.set_ylabel('Count', fontsize=12)\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duration Analysis by Specific Hazard Type\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DURATION ANALYSIS BY SPECIFIC HAZARD TYPE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Extract primary hazard code (first code in list)\n",
    "df_duration['primary_hazard'] = df_duration['hazard_codes'].apply(lambda x: x[0] if x else \"Unknown\")\n",
    "\n",
    "# Get top hazard types\n",
    "top_hazards = df_duration['primary_hazard'].value_counts().head(10)\n",
    "print(f\"\\nTop 10 Hazard Types by Frequency:\")\n",
    "print(top_hazards)\n",
    "\n",
    "# Filter for top hazards\n",
    "df_top_hazards = df_duration[df_duration['primary_hazard'].isin(top_hazards.index)]\n",
    "\n",
    "# Duration statistics by hazard type\n",
    "print(f\"\\n\\nDuration Statistics by Top Hazard Types:\")\n",
    "hazard_duration_stats = df_top_hazards.groupby('primary_hazard')['duration_days'].agg([\n",
    "    'count', 'mean', 'median', 'std', 'min', 'max'\n",
    "]).round(2)\n",
    "hazard_duration_stats = hazard_duration_stats.sort_values('mean', ascending=False)\n",
    "print(hazard_duration_stats)\n",
    "\n",
    "# Visualization: Duration by hazard type with improved scaling\n",
    "fig, ax = plt.subplots(figsize=(16, 7))\n",
    "\n",
    "# Create boxplot with better visibility\n",
    "bp = df_top_hazards.boxplot(\n",
    "    column='duration_days', \n",
    "    by='primary_hazard', \n",
    "    ax=ax,\n",
    "    patch_artist=True,\n",
    "    return_type='dict'\n",
    ")\n",
    "\n",
    "# Customize appearance\n",
    "ax.set_title('Event Duration Distribution by Hazard Type', fontsize=14, fontweight='bold', pad=20)\n",
    "plt.suptitle('')  # Remove default title\n",
    "ax.set_xlabel('Hazard Type', fontsize=12)\n",
    "ax.set_ylabel('Duration (days)', fontsize=12)\n",
    "ax.tick_params(axis='x', rotation=45, labelsize=10)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Auto-scale to show full range with padding\n",
    "max_duration = df_top_hazards['duration_days'].max()\n",
    "ax.set_ylim(-5, max_duration * 1.05)\n",
    "\n",
    "# Add reference line for median overall duration\n",
    "overall_median = df_top_hazards['duration_days'].median()\n",
    "ax.axhline(overall_median, color='red', linestyle='--', linewidth=1, \n",
    "           alpha=0.5, label=f'Overall Median: {overall_median:.1f} days')\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seasonal Duration Patterns\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SEASONAL DURATION PATTERNS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Duration by season\n",
    "season_duration = df_duration.groupby('season')['duration_days'].agg(['mean', 'median', 'count']).round(2)\n",
    "print(f\"\\nDuration by Season:\")\n",
    "print(season_duration)\n",
    "\n",
    "# Duration by season and onset type\n",
    "season_onset_duration = df_duration.groupby(['season', 'onset_type'])['duration_days'].agg(['mean', 'count']).round(2)\n",
    "print(f\"\\n\\nDuration by Season and Onset Type:\")\n",
    "print(season_onset_duration)\n",
    "\n",
    "# Visualization\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot 1: Average duration by season\n",
    "season_duration['mean'].plot(kind='bar', ax=ax1, color='steelblue')\n",
    "ax1.set_title('Average Event Duration by Season', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Season', fontsize=12)\n",
    "ax1.set_ylabel('Average Duration (days)', fontsize=12)\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Heatmap of duration by season and onset type\n",
    "pivot_data = df_duration.pivot_table(values='duration_days', index='season', columns='onset_type', aggfunc='mean')\n",
    "sns.heatmap(pivot_data, annot=True, fmt='.1f', cmap='YlOrRd', ax=ax2, cbar_kws={'label': 'Avg Duration (days)'})\n",
    "ax2.set_title('Average Duration: Season vs Onset Type', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Onset Type', fontsize=12)\n",
    "ax2.set_ylabel('Season', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Country-Level Duration Analysis\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"COUNTRY-LEVEL DURATION ANALYSIS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Top countries by event count\n",
    "top_countries = df_duration['primary_country'].value_counts().head(10)\n",
    "print(f\"\\nTop 10 Countries by Event Count:\")\n",
    "print(top_countries)\n",
    "\n",
    "# Duration by country\n",
    "country_duration = df_duration.groupby('primary_country')['duration_days'].agg([\n",
    "    'count', 'mean', 'median'\n",
    "]).round(2)\n",
    "country_duration = country_duration[country_duration['count'] >= 5]  # At least 5 events\n",
    "country_duration = country_duration.sort_values('mean', ascending=False).head(15)\n",
    "\n",
    "print(f\"\\n\\nTop 15 Countries by Average Duration (min 5 events):\")\n",
    "print(country_duration)\n",
    "\n",
    "# Visualization\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Event count by country\n",
    "top_countries.plot(kind='barh', ax=ax1, color='skyblue')\n",
    "ax1.set_title('Top 10 Countries by Event Count', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Number of Events', fontsize=12)\n",
    "ax1.set_ylabel('Country', fontsize=12)\n",
    "ax1.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# Plot 2: Average duration by country\n",
    "country_duration['mean'].plot(kind='barh', ax=ax2, color='coral')\n",
    "ax2.set_title('Top 15 Countries by Average Event Duration', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Average Duration (days)', fontsize=12)\n",
    "ax2.set_ylabel('Country', fontsize=12)\n",
    "ax2.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disaster Lifecycle Profiles by Hazard Cluster\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DISASTER LIFECYCLE PROFILES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create hazard clusters based on similar characteristics\n",
    "hazard_clusters = {\n",
    "    \"Floods\": [\"FL\", \"FF\", \"MH0600\", \"MH0601\", \"MH0602\", \"MH0603\", \"MH0604\", \n",
    "               \"nat-hyd-flo-flo\", \"nat-hyd-flo-fla\", \"nat-hyd-flo-riv\", \"nat-hyd-flo-coa\"],\n",
    "    \"Tropical Cyclones\": [\"TC\", \"MH0306\", \"MH0307\", \"MH0308\", \"MH0309\", \"nat-met-sto-tro\"],\n",
    "    \"Droughts\": [\"DR\", \"MH0800\", \"MH0801\", \"MH0802\", \"nat-cli-dro\"],\n",
    "    \"Earthquakes\": [\"EQ\", \"MH0100\", \"MH0101\", \"MH0102\", \"nat-geo-tec-ear\"],\n",
    "    \"Temperature Extremes\": [\"HT\", \"CW\", \"nat-cli-ext-hea\", \"nat-cli-ext-col\"]\n",
    "}\n",
    "\n",
    "def assign_cluster(hazard_codes):\n",
    "    \"\"\"Assign hazard to a cluster.\"\"\"\n",
    "    for cluster_name, codes in hazard_clusters.items():\n",
    "        if any(any(code in hc for code in codes) for hc in hazard_codes):\n",
    "            return cluster_name\n",
    "    return \"Other\"\n",
    "\n",
    "df_duration['hazard_cluster'] = df_duration['hazard_codes'].apply(assign_cluster)\n",
    "\n",
    "# Lifecycle profile statistics\n",
    "cluster_profiles = df_duration.groupby('hazard_cluster').agg({\n",
    "    'duration_days': ['count', 'mean', 'median', 'std', 'min', 'max'],\n",
    "    'onset_type': lambda x: x.mode()[0] if len(x.mode()) > 0 else \"Unknown\"\n",
    "}).round(2)\n",
    "\n",
    "print(f\"\\nDisaster Lifecycle Profiles by Hazard Cluster:\")\n",
    "print(cluster_profiles)\n",
    "\n",
    "# Distribution visualization\n",
    "clusters_with_data = df_duration['hazard_cluster'].value_counts()\n",
    "print(f\"\\n\\nEvent Count by Hazard Cluster:\")\n",
    "print(clusters_with_data)\n",
    "\n",
    "# Create lifecycle profile visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for idx, (cluster, group) in enumerate(df_duration.groupby('hazard_cluster')):\n",
    "    if idx >= 4:  # Limit to top 4 clusters\n",
    "        break\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Duration distribution histogram - Clip negative values to 0 for display\n",
    "    group_clipped = group.copy()\n",
    "    group_clipped['duration_days'] = group_clipped['duration_days'].clip(lower=0)\n",
    "    group_clipped['duration_days'].hist(bins=30, ax=ax, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "    \n",
    "    mean_val = group['duration_days'].mean()\n",
    "    median_val = group['duration_days'].median()\n",
    "    \n",
    "    ax.axvline(mean_val, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_val:.1f} days')\n",
    "    ax.axvline(median_val, color='green', linestyle='--', linewidth=2, label=f'Median: {median_val:.1f} days')\n",
    "    \n",
    "    ax.set_title(f'{cluster} Lifecycle Profile (n={len(group)})', fontsize=12, fontweight='bold')\n",
    "    ax.set_xlabel('Duration (days)', fontsize=10)\n",
    "    ax.set_ylabel('Frequency', fontsize=10)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.set_xlim(left=0)  # Set x-axis to start at 0 (no negative values displayed)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Duration Trend Over Time\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DURATION TRENDS OVER TIME\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Add year column\n",
    "df_duration['year'] = df_duration['start_datetime'].dt.year\n",
    "\n",
    "# Average duration by year\n",
    "yearly_duration = df_duration.groupby('year')['duration_days'].agg(['mean', 'median', 'count']).round(2)\n",
    "print(f\"\\nAverage Duration by Year:\")\n",
    "print(yearly_duration)\n",
    "\n",
    "# Visualization\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Duration trend over years\n",
    "ax1.plot(yearly_duration.index, yearly_duration['mean'], marker='o', linewidth=2, \n",
    "         markersize=8, color='steelblue', label='Mean Duration')\n",
    "ax1.plot(yearly_duration.index, yearly_duration['median'], marker='s', linewidth=2, \n",
    "         markersize=8, color='coral', label='Median Duration')\n",
    "ax1.set_title('Event Duration Trend Over Years', fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Year', fontsize=12)\n",
    "ax1.set_ylabel('Duration (days)', fontsize=12)\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Event count over years\n",
    "yearly_duration['count'].plot(kind='bar', ax=ax2, color='lightgreen', alpha=0.7)\n",
    "ax2.set_title('Event Count by Year', fontsize=14, fontweight='bold')\n",
    "ax2.set_xlabel('Year', fontsize=12)\n",
    "ax2.set_ylabel('Number of Events', fontsize=12)\n",
    "ax2.tick_params(axis='x', rotation=45)\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Duration trend by hazard cluster over time\n",
    "cluster_year_duration = df_duration.groupby(['year', 'hazard_cluster'])['duration_days'].mean().unstack()\n",
    "print(f\"\\n\\nAverage Duration by Year and Hazard Cluster:\")\n",
    "print(cluster_year_duration.round(2))\n",
    "\n",
    "# Plot duration trends by cluster with symlog scale (linear near zero, log elsewhere)\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "# Plot all clusters except Droughts\n",
    "for cluster in cluster_year_duration.columns:\n",
    "    if cluster != 'Droughts':  # Skip Droughts\n",
    "        ax.plot(cluster_year_duration.index, cluster_year_duration[cluster], \n",
    "                marker='o', linewidth=2.5, label=cluster, markersize=7)\n",
    "\n",
    "ax.set_title('Duration Trends by Hazard Cluster Over Time (Symmetrical Log Scale)', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Year', fontsize=12)\n",
    "ax.set_ylabel('Average Duration (days)', fontsize=12)\n",
    "\n",
    "# Use symlog scale: linear for small values (0-0.5), log elsewhere\n",
    "# linthresh=0.5 means values between -0.5 and +0.5 are linear\n",
    "ax.set_yscale('symlog', linthresh=0.5)\n",
    "\n",
    "# Format y-axis to show actual numbers instead of scientific notation\n",
    "ax.yaxis.set_major_formatter(FuncFormatter(lambda y, _: f'{int(y) if y >= 1 else f\"{y:.1f}\"}'))\n",
    "\n",
    "ax.legend(loc='best', fontsize=11)\n",
    "ax.grid(True, alpha=0.3, which='both')  # Show grid for both major and minor ticks\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. **Time-series extraction** from STAC collections using CQL2 date filters\n",
    "2. **Seasonality visualization** for hazard types\n",
    "3. **Statistical trend analysis** on impact data\n",
    "4. **Event duration analysis** segmented by hazard type, season, and country\n",
    "5. **Disaster lifecycle profiles** for different hazard clusters\n",
    "\n",
    "**Key Findings from Duration Analysis**:\n",
    "- Sudden-onset events (earthquakes, cyclones) typically have shorter durations\n",
    "- Slow-onset events (floods, droughts) show longer duration patterns\n",
    "- Seasonal variations affect event duration significantly\n",
    "- Country-specific duration patterns reveal regional vulnerability characteristics\n",
    "\n",
    "**Next Steps:**\n",
    "- Integrate Correlation IDs to normalize trends\n",
    "- Cross-Correlation between Hazard Magnitude and Impact Severity\n",
    "- Build regression models to predict event duration\n",
    "- Add economic/income level data for enhanced analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fast_api_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
